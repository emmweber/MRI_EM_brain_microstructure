{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e1cec2",
   "metadata": {},
   "source": [
    "# Split the synthetic and experimental datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79cbb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines imports\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import pickle \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13f277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines constants\n",
    "_DATA_FOLDER = '/home/gustavo/Gdrive/Stanford/Lab/ODF_prediction/data/imagenet_new'\n",
    "_PERCENTAGE_TRAIN = 0.8\n",
    "_PERCENTAGE_VAL = 0.1\n",
    "_NUM_SAMPLES = 400\n",
    "_SNRS_DB = [30,40,50,90]\n",
    "_NUM_NOISE_LEVELS = len(_SNRS_DB)\n",
    "_ALL_SYNTHETIC_DATASETS = ['dog','goldfish','hotpot','panda','sealion','terrier']\n",
    "_COMBINATIONS = [['dog'],\n",
    "    ['dog','sealion'],\n",
    "    ['dog','panda'],\n",
    "    ['dog','sealion','panda'],\n",
    "    ['dog','goldfish','hotpot','panda','sealion','terrier'],\n",
    "    ['goldfish','hotpot','panda','sealion','terrier']]\n",
    "# Random seed to have replicable results\n",
    "_RANDOM_STATE = 20181970\n",
    "_OUT_FOLDER = '/home/gustavo/Gdrive/Stanford/Lab/ODF_prediction/datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a97bd",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f24b29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(os.path.exists(_OUT_FOLDER)):\n",
    "    os.mkdir(_OUT_FOLDER)\n",
    "synthetic_out_folder = os.path.join(_OUT_FOLDER,'synthetic')\n",
    "experimental_out_folder = os.path.join(_OUT_FOLDER,'experimental')\n",
    "if not(os.path.exists(synthetic_out_folder)):\n",
    "    os.mkdir(synthetic_out_folder)\n",
    "if not(os.path.exists(experimental_out_folder)):\n",
    "    os.mkdir(experimental_out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f24a2e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_30dB\n",
      "dog_40dB\n",
      "dog_50dB\n",
      "dog_90dB\n",
      "goldfish_30dB\n",
      "goldfish_40dB\n",
      "goldfish_50dB\n",
      "goldfish_90dB\n",
      "hotpot_30dB\n",
      "hotpot_40dB\n",
      "hotpot_50dB\n",
      "hotpot_90dB\n",
      "panda_30dB\n",
      "panda_40dB\n",
      "panda_50dB\n",
      "panda_90dB\n",
      "sealion_30dB\n",
      "sealion_40dB\n",
      "sealion_50dB\n",
      "sealion_90dB\n",
      "terrier_30dB\n",
      "terrier_40dB\n",
      "terrier_50dB\n",
      "terrier_90dB\n"
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "\n",
    "for basename in _ALL_SYNTHETIC_DATASETS:\n",
    "    for noise_level in _SNRS_DB:\n",
    "        all_data[basename+'_'+str(noise_level)+'dB']={'groundtruth_file': os.path.join(_DATA_FOLDER, basename+'_groundtruth.mat'),\n",
    "                        'mt_file': os.path.join(_DATA_FOLDER,basename+'_MT_'+str(noise_level)+'dB.mat'),\n",
    "                        'diffusion_file': os.path.join(_DATA_FOLDER,basename+'_diffusion_'+str(noise_level)+'dB_normalized.mat')}\n",
    "\n",
    "for key in all_data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e99597",
   "metadata": {},
   "source": [
    "### Load and format groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332e00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_groundtruth(groundtruth_file):\n",
    "    grountruth_contents = scio.loadmat(groundtruth_file)\n",
    "    img_fitted_params_groundtruth = np.concatenate((grountruth_contents['axon_fit_params'],grountruth_contents['gratio_fit_params']),axis=2)\n",
    "    brain_mask = grountruth_contents['mask']\n",
    "    img_gratio_samples = grountruth_contents['gratio_samples']\n",
    "    img_axon_samples = grountruth_contents['axon_samples']\n",
    "    img_fractions = grountruth_contents['fractions_groundtruth']\n",
    "    height, width = brain_mask.shape\n",
    "    list_indices = []\n",
    "    gratio_samples = []\n",
    "    diameter_samples = []\n",
    "    fractions_samples = []\n",
    "    # groundtruth parameters that come from fitting the histology samples directly\n",
    "    # order is alpha, beta (matlab parametrization of Gamma)\n",
    "    fitted_params = []\n",
    "    for idx_y in range(height):\n",
    "        for idx_x in range(width):\n",
    "            if brain_mask[idx_y,idx_x]:\n",
    "                # saves the index position in the image for future reconstruction\n",
    "                list_indices.append(np.array([idx_y,idx_x]))\n",
    "                # extracts samples and fractions\n",
    "                gratio_samples.append(np.squeeze(img_gratio_samples[idx_y,idx_x,:]))\n",
    "                diameter_samples.append(np.squeeze(img_axon_samples[idx_y,idx_x,:]))\n",
    "                fractions_samples.append(np.squeeze(img_fractions[idx_y,idx_x,:]))\n",
    "                fitted_params.append(np.squeeze(img_fitted_params_groundtruth[idx_y,idx_x,:]))\n",
    "    gratio_samples = np.array(gratio_samples)\n",
    "    diameter_samples = np.array(diameter_samples)\n",
    "    fractions_samples = np.array(fractions_samples)\n",
    "    fitted_params = np.array(fitted_params)\n",
    "    num_voxels = len(list_indices)\n",
    "    print(f'{num_voxels} voxels available')\n",
    "    return list_indices, gratio_samples, diameter_samples, fractions_samples, fitted_params\n",
    "\n",
    "def load_mri(diff_file, mt_files, list_indices):\n",
    "    # Loads the MT images\n",
    "    MT_data = scio.loadmat(mt_files)\n",
    "    img_mt = MT_data['simulated_MTs']\n",
    "    # Loads the diffusion images\n",
    "    Diff_data = scio.loadmat(diff_file)\n",
    "    img_diff = Diff_data['diffusion_normalized']\n",
    "    diff_samples = []\n",
    "    mt_samples = []\n",
    "    for valid_index in list_indices:\n",
    "        idx_y = valid_index[0]\n",
    "        idx_x = valid_index[1]\n",
    "        diff_samples.append(np.squeeze(img_diff[idx_y,idx_x,:]))\n",
    "        mt_samples.append(np.squeeze(img_mt[idx_y,idx_x,:]))\n",
    "    diff_samples = np.array(diff_samples)\n",
    "    mt_samples = np.array(mt_samples)\n",
    "    x_all = np.concatenate((diff_samples,mt_samples),axis=1)\n",
    "    num_features = x_all.shape[1]\n",
    "    print(f'{num_features} MRI features')\n",
    "    return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea5ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_30dB\n",
      "1095 voxels available\n",
      "86 MRI features\n",
      "dog_40dB\n",
      "1095 voxels available\n",
      "86 MRI features\n",
      "dog_50dB\n",
      "1095 voxels available\n",
      "86 MRI features\n",
      "dog_90dB\n",
      "1095 voxels available\n",
      "86 MRI features\n",
      "goldfish_30dB\n",
      "7755 voxels available\n",
      "86 MRI features\n",
      "goldfish_40dB\n",
      "7755 voxels available\n",
      "86 MRI features\n",
      "goldfish_50dB\n",
      "7755 voxels available\n",
      "86 MRI features\n",
      "goldfish_90dB\n",
      "7755 voxels available\n",
      "86 MRI features\n",
      "hotpot_30dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "hotpot_40dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "hotpot_50dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "hotpot_90dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "panda_30dB\n",
      "7877 voxels available\n",
      "86 MRI features\n",
      "panda_40dB\n",
      "7877 voxels available\n",
      "86 MRI features\n",
      "panda_50dB\n",
      "7877 voxels available\n",
      "86 MRI features\n",
      "panda_90dB\n",
      "7877 voxels available\n",
      "86 MRI features\n",
      "sealion_30dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "sealion_40dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "sealion_50dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "sealion_90dB\n",
      "7990 voxels available\n",
      "86 MRI features\n",
      "terrier_30dB\n",
      "7890 voxels available\n",
      "86 MRI features\n",
      "terrier_40dB\n",
      "7890 voxels available\n",
      "86 MRI features\n",
      "terrier_50dB\n",
      "7890 voxels available\n",
      "86 MRI features\n",
      "terrier_90dB\n",
      "7890 voxels available\n",
      "86 MRI features\n"
     ]
    }
   ],
   "source": [
    "for key, info in all_data.items():\n",
    "    print(key)\n",
    "    list_indices, gratio_samples, diameter_samples, fractions_samples, fitted_params = load_groundtruth(info['groundtruth_file'])\n",
    "    info['gratio_samples'] = gratio_samples[:,:_NUM_SAMPLES]\n",
    "    info['diameter_samples'] = diameter_samples[:,:_NUM_SAMPLES]\n",
    "    info['fractions_samples'] = fractions_samples\n",
    "    info['fitted_params'] = fitted_params\n",
    "    # Loads all noise levels\n",
    "    x_all = load_mri(info['diffusion_file'], info['mt_file'], list_indices)\n",
    "    info['mri_data'] = x_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac514a4",
   "metadata": {},
   "source": [
    "### Divide in training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "198b9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data(data_struct,sel_keys,field):\n",
    "    list_data = []\n",
    "    for key, item in data_struct.items():\n",
    "        if key in sel_keys:\n",
    "            list_data.append(item[field])\n",
    "    return np.concatenate(list_data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dca5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 876 for training\n",
      "Using 109 for validation\n",
      "Using 110 for testing\n",
      "====dog=====\n",
      "train: (3504, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (436, 86)\n",
      "x_val min: 0.0\n",
      "x_val max: 1.0000000000000004\n",
      "test: (440, 86)\n",
      "x_test min: 0.0\n",
      "x_test max: 1.0000000000000004\n",
      "Using 3633 for training\n",
      "Using 454 for validation\n",
      "Using 455 for testing\n",
      "====dog_sealion=====\n",
      "train: (29064, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (3632, 86)\n",
      "x_val min: 0.0\n",
      "x_val max: 1.0000000000000004\n",
      "test: (3640, 86)\n",
      "x_test min: 0.0\n",
      "x_test max: 1.0000000000000004\n",
      "Using 3588 for training\n",
      "Using 448 for validation\n",
      "Using 450 for testing\n",
      "====dog_panda=====\n",
      "train: (28704, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (3584, 86)\n",
      "x_val min: 0.0\n",
      "x_val max: 1.0000000000000004\n",
      "test: (3600, 86)\n",
      "x_test min: 0.0\n",
      "x_test max: 1.0000000000000004\n",
      "Using 4523 for training\n",
      "Using 565 for validation\n",
      "Using 566 for testing\n",
      "====dog_sealion_panda=====\n",
      "train: (54276, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (6780, 86)\n",
      "x_val min: 0.0\n",
      "x_val max: 1.0000000000000004\n",
      "test: (6792, 86)\n",
      "x_test min: 0.0\n",
      "x_test max: 1.0000000000000004\n",
      "Using 5412 for training\n",
      "Using 676 for validation\n",
      "Using 678 for testing\n",
      "====dog_goldfish_hotpot_panda_sealion_terrier=====\n",
      "train: (129888, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (16224, 86)\n",
      "x_val min: 0.0\n",
      "x_val max: 1.0000000000000004\n",
      "test: (16272, 86)\n",
      "x_test min: 0.0\n",
      "x_test max: 1.0000000000000004\n",
      "Using 6320 for training\n",
      "Using 790 for validation\n",
      "Using 790 for testing\n",
      "====goldfish_hotpot_panda_sealion_terrier=====\n",
      "train: (126400, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (15800, 86)\n",
      "x_val min: 0.0\n",
      "x_val max: 1.0000000000000004\n",
      "test: (15800, 86)\n",
      "x_test min: 0.0\n",
      "x_test max: 1.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Divides the datasets into train, validation and testing.\n",
    "splits = ['train','val','test']\n",
    "for combination_basenames in _COMBINATIONS:\n",
    "    \n",
    "    # Initializes data structures\n",
    "    data_save = {}\n",
    "    indices = {}\n",
    "    for split in splits:\n",
    "        data_save[split] = {}\n",
    "        indices[split] = []\n",
    "        \n",
    "    all_names = []\n",
    "    name_dataset = '_'.join(combination_basenames)\n",
    "    for basename in combination_basenames:\n",
    "        for noise_level in _SNRS_DB:\n",
    "            all_names.append(basename+'_'+str(noise_level)+'dB')\n",
    "\n",
    "    x_all = concatenate_data(all_data,all_names, 'mri_data')\n",
    "    diameter_samples = concatenate_data(all_data, all_names, 'diameter_samples')\n",
    "    gratio_samples = concatenate_data(all_data, all_names, 'gratio_samples')\n",
    "    fractions_samples = concatenate_data(all_data, all_names, 'fractions_samples')\n",
    "    fitted_params = concatenate_data(all_data, all_names, 'fitted_params')\n",
    "    num_features = x_all.shape[1]\n",
    "    num_voxels = x_all.shape[0]\n",
    "    \n",
    "    num_voxels_per_snr = int(num_voxels/len(all_names))\n",
    "    shuffled_indices = list(range(num_voxels_per_snr))\n",
    "    random.Random(_RANDOM_STATE).shuffle(shuffled_indices)\n",
    "    num_train = int(_PERCENTAGE_TRAIN*num_voxels_per_snr)\n",
    "    num_val = int(_PERCENTAGE_VAL*num_voxels_per_snr)\n",
    "    num_test = num_voxels_per_snr - num_train - num_val\n",
    "\n",
    "    indices_train = []\n",
    "    indices_val = []\n",
    "    indices_test = []\n",
    "\n",
    "    for ii in range(len(all_names)):\n",
    "        start = ii*num_voxels_per_snr\n",
    "        indices['train'] += list(start+np.array(shuffled_indices[:num_train]))\n",
    "        indices['val'] += list(start+np.array(shuffled_indices[num_train:(num_train+num_val)]))\n",
    "        indices['test'] += list(start+np.array(shuffled_indices[(num_train+num_val):]))\n",
    "\n",
    "    print(f'Using {num_train} for training')\n",
    "    print(f'Using {num_val} for validation')\n",
    "    print(f'Using {num_test} for testing')\n",
    "    \n",
    "    # Scaler of features\n",
    "    x_train = x_all[indices['train'],:]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit_transform(x_train)\n",
    "\n",
    "    print(f'===={name_dataset}=====')\n",
    "    for split in splits:\n",
    "        split_indices = indices[split]\n",
    "        data_save[split]['scaler'] = scaler\n",
    "        data_save[split]['indices'] = split_indices\n",
    "        data_save[split]['gratio_samples'] = gratio_samples[split_indices,:]\n",
    "        data_save[split]['diameter_samples'] = diameter_samples[split_indices,:]\n",
    "        data_save[split]['fractions_samples'] = fractions_samples[split_indices,:]\n",
    "        data_save[split]['params_gt'] = fitted_params[split_indices,:]\n",
    "        x = x_all[split_indices,:]\n",
    "        print(split+': '+str(x.shape))\n",
    "        x_scaled = scaler.transform(x)\n",
    "        print(f'x_{split} min: {x_scaled.min()}')\n",
    "        print(f'x_{split} max: {x_scaled.max()}')\n",
    "        data_save[split]['x_unnormalized'] = x\n",
    "        data_save[split]['x_scaled'] = x_scaled\n",
    "    with open(os.path.join(synthetic_out_folder,f'{name_dataset}.pkl'), 'wb') as file:\n",
    "        pickle.dump(data_save, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d6936",
   "metadata": {},
   "source": [
    "## Experimental dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd01ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_FOLDER = '/home/gustavo/Gdrive/Stanford/Lab/ODF_prediction/data'\n",
    "_MOUSE_DATA = [os.path.join(_DATA_FOLDER,'mouse_seizure_data_genu.mat'),\n",
    "os.path.join(_DATA_FOLDER,'mouse_seizure_data_body.mat'),\n",
    "os.path.join(_DATA_FOLDER,'mouse_seizure_data_splenium.mat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8279e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mouse = []\n",
    "axon_fit_params_mouse = []\n",
    "axon_samples_mouse = []\n",
    "fractions_mouse = []\n",
    "gratio_fit_params_mouse = []\n",
    "gratio_samples_mouse = []\n",
    "groundtruth_params = []\n",
    "for name in _MOUSE_DATA:\n",
    "    mouse_data = scio.loadmat(name)\n",
    "    x_mouse.append(mouse_data['MRI_inputs'])\n",
    "    axon_fit_params_mouse.append(mouse_data['axon_fit_params'])\n",
    "    axon_samples_mouse.append(mouse_data['axon_samples'])\n",
    "    fractions_mouse.append(mouse_data['fractions_groundtruth'])\n",
    "    gratio_fit_params_mouse.append(mouse_data['gratio_fit_params'])\n",
    "    gratio_samples_mouse.append(mouse_data['gratio_samples'])\n",
    "\n",
    "x_mouse = np.concatenate(x_mouse,axis=0)\n",
    "axon_fit_params_mouse = np.concatenate(axon_fit_params_mouse,axis=0)\n",
    "axon_samples_mouse = np.concatenate(axon_samples_mouse,axis=0)\n",
    "fractions_mouse = np.concatenate(fractions_mouse,axis=0)\n",
    "gratio_fit_params_mouse = np.concatenate(gratio_fit_params_mouse,axis=0)\n",
    "gratio_samples_mouse = np.concatenate(gratio_samples_mouse,axis=0)   \n",
    "groundtruth_params=np.concatenate((axon_fit_params_mouse,gratio_fit_params_mouse), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e58e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (14, 86)\n",
      "x_train min: 0.0\n",
      "x_train max: 1.0000000000000004\n",
      "val: (6, 86)\n",
      "x_val min: -0.6289138429481089\n",
      "x_val max: 1.8381362372257237\n",
      "test: (6, 86)\n",
      "x_test min: -0.9860273451163553\n",
      "x_test max: 1.3754582067145866\n"
     ]
    }
   ],
   "source": [
    "# Got separation from random in matlab, we maintain 1 HET and 1 WT in each validation and testing\n",
    "ind_train_base = [1,8,6,2,0]\n",
    "ind_val_base = [3,4]\n",
    "ind_test_base = [5,7]\n",
    "\n",
    "ind_train = []\n",
    "ind_val = []\n",
    "ind_test = []\n",
    "num_voxels_per_snr = 9\n",
    "for ii in range(len(_MOUSE_DATA)):\n",
    "    start = ii*num_voxels_per_snr\n",
    "    ind_train += list(start+np.array(ind_train_base))\n",
    "    ind_val += list(start+np.array(ind_val_base))\n",
    "    ind_test += list(start+np.array(ind_test_base))\n",
    "\n",
    "# Remove last sample of training set (there was a problem with that mouse in the splenium)\n",
    "ind_train.pop()    \n",
    "splits = ['train','val','test']\n",
    "\n",
    "data_mouse = {}\n",
    "indices = {}\n",
    "for split in splits:\n",
    "    data_mouse[split] = {}\n",
    "indices['train'] = np.array(ind_train)\n",
    "indices['val'] = np.array(ind_val)\n",
    "indices['test'] = np.array(ind_test)\n",
    "\n",
    "x_mouse_train = x_mouse[indices['train'],:]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(x_mouse_train)\n",
    "\n",
    "for split in splits:\n",
    "    split_indices = indices[split]\n",
    "    data_mouse[split]['scaler'] = scaler\n",
    "    data_mouse[split]['indices'] = split_indices\n",
    "    data_mouse[split]['gratio_samples'] = gratio_samples_mouse[split_indices,:]\n",
    "    data_mouse[split]['diameter_samples'] = axon_samples_mouse[split_indices,:]\n",
    "    data_mouse[split]['fractions_samples'] = fractions_mouse[split_indices,:]\n",
    "    data_mouse[split]['params_gt'] = groundtruth_params[split_indices,:]\n",
    "    x = x_mouse[split_indices,:]\n",
    "    x_scaled = scaler.transform(x)\n",
    "    print(split+': '+str(x.shape))\n",
    "    print(f'x_{split} min: {x_scaled.min()}')\n",
    "    print(f'x_{split} max: {x_scaled.max()}')\n",
    "    data_mouse[split]['x_unnormalized'] = x\n",
    "    data_mouse[split]['x_scaled'] = x_scaled\n",
    "    with open(os.path.join(experimental_out_folder,'mouse_3regions.pkl'), 'wb') as file:\n",
    "        pickle.dump(data_mouse, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
